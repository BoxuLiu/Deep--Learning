# -*- coding: utf-8 -*-
"""Assignment _1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/180BFrOVNUb-g6Y2HmoQsAZhC1CZGYRiz
"""

!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz

!mv cifar-10-python.tar.gz ./gdrive/MyDrive/DirName/Datasets/

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/MyDrive/DirName/Datasets
!tar xvfz cifar-10-python.tar.gz
!ls

import pickle
import matplotlib.pyplot as plt
import cupy as np
import random

def LoadBatch(filename, len = None):
    with open(filename, 'rb') as fo:
        dataDict = pickle.load(fo, encoding='bytes')
        X = (dataDict[b"data"] / 255).T
        y = dataDict[b"labels"]
        Y = (np.eye(10)[y]).T
    # return X[:, :len], Y[:, :len], y[:len]
    return np.asarray(X[:, :len]), np.asarray(Y[:, :len]), np.asarray(y[:len])

def PreProcess(data):
    for i in range(0, len(data[0, :])):
      data[:, i] = (data[:, i] - np.mean(data[:, i]))/ np.std(data[:, i])
    # return data
    return np.asarray(data)

def Argumentation(data):
    for i in range(0, len(data[0, :])):
      if random.randint(0,9) >=5:
        img_tem = np.reshape(data[:,i],(3,32,32))
        img_tem = np.fliplr(img_tem)
        data[:, i] = np.reshape(img_tem,(3072,))
    return np.asarray(data)


def InitialParameters(data, lables):
    W = np.random.normal(0, 0.01, (len(lables), data.shape[0]))
    b = np.random.normal(0, 0.01, (len(lables), 1))
    # print(W.shape)
    return W, b

def EvaluateClassifier(X, W, b, func = "softmax"):
    s = W@X + b
    if func == "softmax":
      p = np.exp(s) / np.sum(np.exp(s), axis = 0)
      return p
    elif func == "sigmod":
      p = np.exp(s) / (np.exp(s) + 1)
      return p

def ComputeCost(X, Y, W, b, l, func = "softmax"):
    P = EvaluateClassifier(X, W, b, func)
    if func == "softmax":
      c = -np.sum(Y*np.log(P))/X.shape[1] + l * np.sum(W**2)
    elif func == "sigmod":
      c = -np.sum((1-Y)*np.log(1-P) + Y*np.log(P))/Y.shape[0]/X.shape[1]
    return c

def ComputeAccuracy(X, y, W, b, func = "softmax"):
    p = EvaluateClassifier(X, W, b, func)
    argMaxP = np.argmax(p, axis=0)
    acc = argMaxP.T[argMaxP == np.asarray(y)].shape[0] / X.shape[1]
    return acc

def ComputeGradients(X, Y, P, W, l, func = "softmax"):
    if func == "softmax":
      G = -(Y - P)
    elif func == "sigmod":
      G = -(Y - P)/Y.shape[0]
      
    b_g = np.reshape(G@np.ones(X.shape[1]) / X.shape[1], (Y.shape[0],1))
    W_g = 1 / X.shape[1] * G@X.T + 2 * l * W

    return [W_g, b_g]  

def ComputeGradsNum(X, Y, P, W, b, lamda, h = 1e-6):
    no  =  W.shape[0]
    d  =  X.shape[0]
    grad_W = np.zeros(W.shape);
    grad_b = np.zeros((no, 1));
    c = ComputeCost(X, Y, W, b, lamda);
    for i in range(len(b)):
      b_try = np.array(b)
      b_try[i] += h
      c2 = ComputeCost(X, Y, W, b_try, lamda)
      grad_b[i] = (c2-c) / h
    for i in range(W.shape[0]):
      for j in range(W.shape[1]):
        W_try = np.array(W)
        W_try[i,j] += h
        c2 = ComputeCost(X, Y, W_try, b, lamda)
        grad_W[i,j] = (c2-c) / h
    return [grad_W, grad_b]

def shuffle(X, Y):
    con = np.concatenate((X, Y),axis=0).T
    np.random.shuffle(con)
    X = con[:, :3072].T
    Y = con[:, 3072:].T
    return X, Y

def PlotPerformance(n_epochs, costs_training, costs_val, title = ""):
    epochs = np.asnumpy(np.arange(n_epochs))

    fig, ax = plt.subplots(figsize=(10, 8))
    ax.plot(epochs, costs_training, label="Training set")
    ax.plot(epochs, costs_val, label="Validation set")
    ax.legend()
    ax.set(xlabel='Number of epochs', ylabel='Cost')
    ax.grid()

    # plt.savefig("/content/drive/MyDrive/DirName/Result Pics/" + title + ".png", bbox_inches="tight")

def MiniBatchGD(X, Y, W, b, l = 0.1, n_batches = 100, eta = 0.0025, epochs = 40, X_validation = None, Y_validation = None, y_validation = None, func = "softmax"):
    cost_training = np.zeros(epochs)
    cost_validation = np.zeros(epochs)

    for i in range(epochs):
      X, Y = shuffle(X, Y)
      print("epoch:", i+1)
      for j in range(1, int(X.shape[1]/n_batches) + 1):
        j_start = (j - 1) * n_batches
        j_end = j * n_batches
        batch_x = X[:, j_start:j_end]
        batch_y = Y[:, j_start:j_end]
        p = EvaluateClassifier(batch_x, W, b, func = func)
        W_g, b_g = ComputeGradients(batch_x, batch_y, p, W, l, func = func)
        W -= eta * W_g
        b -= eta * b_g

        cost_training[i] = ComputeCost(X, Y, W, b, l, func = func)
        cost_validation[i] = ComputeCost(X_validation, Y_validation, W, b, l, func = func)

      # if i % 20 == 0 and i != 0:
      #   eta *= 0.7

    print("training accuracy:", ComputeAccuracy(data, batch[2], parameters[0], parameters[1], func = func))
    print("test accuracy:", ComputeAccuracy(X_validation, y_validation, parameters[0], parameters[1], func = func))
    PlotPerformance(epochs, np.asnumpy(cost_training), np.asnumpy(cost_validation), "lambda=0,epochs=40,batch=100,eta=1")

def montage(W):
    fig, ax = plt.subplots(2,5)
    for i in range(2):
      for j in range(5):
        im  = W[i*5+j,:].reshape(32,32,3, order='F')
        sim = (im-np.min(im[:]))/(np.max(im[:])-np.min(im[:]))
        sim = sim.transpose(1,0,2)
        ax[i][j].imshow(sim, interpolation='nearest')
        ax[i][j].set_title("y="+str(5*i+j))
        ax[i][j].axis('off')
    plt.show()

def LoadAllBatch(path, len = None):
    batches = LoadBatch(path + str(1))
    for i in range(2,6):
      batch_tem = LoadBatch(path + str(i))
      batches = (np.concatenate((batch_tem[0], batches[0]),axis=1), np.concatenate((batch_tem[1], batches[1]),axis=1), np.concatenate((batch_tem[2], batches[2]),axis=0))
    batches = (batches[0][:, :len], batches[1][:, :len], batches[2][:len])
    return batches

batch = LoadAllBatch('/content/drive/MyDrive/DirName/Datasets/cifar-10-batches-py/data_batch_',10000)
data = PreProcess(batch[0])
batch_validation = LoadBatch('/content/drive/MyDrive/DirName/Datasets/cifar-10-batches-py/test_batch')
data_validation = PreProcess(batch_validation[0])

# data = Argumentation(data)
parameters = InitialParameters(data,batch[1])
MiniBatchGD(data, batch[1], parameters[0], parameters[1], X_validation = data_validation, Y_validation = batch_validation[1], y_validation = batch_validation[2], func = "sigmod")
# montage(np.asnumpy(parameters[0]))

parameters_sig = InitialParameters(data,batch[1])
MiniBatchGD(data, batch[1], parameters_sig[0], parameters_sig[1], X_validation = data_validation, Y_validation = batch_validation[1], y_validation = batch_validation[2], func = "softmax")

print("training accuracy:", ComputeAccuracy(data, batch[2], parameters_sig[0], parameters_sig[1], func = "sigmod"))
print("test accuracy:", ComputeAccuracy(data_validation, batch_validation[2], parameters_sig[0], parameters_sig[1], func = "sigmod"))

p = EvaluateClassifier(data_validation, parameters_sig[0], parameters_sig[1], func = "sigmod")
argMaxP = np.argmax(p, axis=0)
correct = np.asnumpy(argMaxP[argMaxP == np.asarray(batch_validation[2])])
incorrect = np.asnumpy(argMaxP[argMaxP != np.asarray(batch_validation[2])])

plt.hist(correct, bins = [0,1,2,3,4,5,6,7,8,9]) 
plt.title("correct") 
plt.show()

plt.hist(incorrect, bins = [0,1,2,3,4,5,6,7,8,9]) 
plt.title("incorrect") 
plt.show()

def EvaluateClassifier(X, W, b, func = "softmax"):
    # print("W", W.shape)
    # print("X", X.shape)
    # print("b", b.shape)
    s = W@X + b
    if func == "softmax":
      p = np.exp(s) / np.sum(np.exp(s), axis = 0)
      return p
    elif func == "sigmod":
      p = np.exp(s) / (np.exp(s) + 1)
      return p
def ComputeGradients(X, Y, P, W, l, func = "softmax"):
    if func == "softmax":
      G = -(Y - P)
    elif func == "sigmod":
      G = -(Y - P)/Y.shape[0]
      
    b_g = np.reshape(G@np.ones(X.shape[1]) / X.shape[1], (Y.shape[0],1))
    W_g = 1 / X.shape[1] * G@X.T + 2 * l * W

    return [W_g, b_g]
def ComputeGradsNum(X, Y, P, W, b, lamda, h = 1e-6,  func = "Softmax"):
    no  =  W.shape[0]
    d  =  X.shape[0]
    grad_W = np.zeros(W.shape);
    grad_b = np.zeros((no, 1));
    c = ComputeCost(X, Y, W, b, lamda, func);
    for i in range(len(b)):
      b_try = np.array(b)
      b_try[i] += h
      c2 = ComputeCost(X, Y, W, b_try, lamda, func)
      grad_b[i] = (c2-c) / h
    for i in range(W.shape[0]):
      for j in range(W.shape[1]):
        W_try = np.array(W)
        W_try[i,j] += h
        c2 = ComputeCost(X, Y, W_try, b, lamda, func)
        grad_W[i,j] = (c2-c) / h
    return [grad_W, grad_b]

batch = LoadAllBatch('/content/drive/MyDrive/DirName/Datasets/cifar-10-batches-py/data_batch_',1)
data = PreProcess(batch[0])

P_t = EvaluateClassifier(data, parameters[0], parameters[1], func = "sigmod")
G_ana = ComputeGradients(data, batch[1], P_t, parameters[0], 0, func = "sigmod")
print(G_ana)
G_num = ComputeGradsNum(data, batch[1], P_t, parameters[0], parameters[1], 0, func = "sigmod")
print(G_num)
print(np.sum(G_ana[0]-G_num[0]))
print(np.sum(G_ana[1]-G_num[1]))