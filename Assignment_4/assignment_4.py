# -*- coding: utf-8 -*-
"""assignment_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mmW2ojAk6BsNKhYcDZur5RGue4Z-YqvK
"""

def compute_gradient_num(X, Y, W, U, V, b, c, h = 1e-6, G = "W"):


      G_W = cp.zeros(W.shape)
      G_U = cp.zeros(U.shape)
      G_V = cp.zeros(V.shape)
      G_b = cp.zeros(b.shape)
      G_c = cp.zeros(c.shape)

      loss = self.compute_loss(X = X, Y = Y, U = U, V = V, W = W, b = b, c = c)
      if G == "W":
        for i in range(W.shape[0]):
          for j in range(W.shape[1]):
            # print(W.shape[0])
            # print(W.shape[1])
            W[i,j] += h
            loss_h = self.compute_loss(X = X, Y = Y, U = U, V = V, W = W, b = b, c = c)
            W[i,j] -= h
            G_W[i,j] = (loss_h - loss) / h
        return G_W
      elif G == "U":
        for i in range(U.shape[0]):
          for j in range(U.shape[1]):
            U[i,j] += h
            loss_h = self.compute_loss(X = X, Y = Y, U = U, V = V, W = W, b = b, c = c)
            U[i,j] -= h
            G_U[i,j] = (loss_h - loss) / h
        return G_U
      elif G == "V":
        for i in range(V.shape[0]):
          for j in range(V.shape[1]):
            V[i,j] += h
            loss_h = self.compute_loss(X = X, Y = Y, U = U, V = V, W = W, b = b, c = c)
            V[i,j] -= h
            G_V[i,j] = (loss_h - loss) / h
        return G_V
      elif G == "b":
        for i in range(b.shape[0]):
          for j in range(b.shape[1]):
            b[i,j] += h
            loss_h = self.compute_loss(X = X, Y = Y, U = U, V = V, W = W, b = b, c = c)
            b[i,j] -= h
            G_b[i,j] = (loss_h - loss) / h
        return G_b
      elif G == "c":
        for i in range(c.shape[0]):
          for j in range(c.shape[1]):
            c[i,j] += h
            loss_h = self.compute_loss(X = X, Y = Y, U = U, V = V, W = W, b = b, c = c)
            c[i,j] -= h
            G_c[i,j] = (loss_h - loss) / h
        return G_c

import numpy as np
import numpy as cp
import matplotlib.pyplot as plt
import random
import copy
from collections import OrderedDict
import queue 

def LoadText(filename):
  book_data = open(filename, 'r', encoding='utf8').read()
  chars = list(set(book_data))
  # chars = ["@"] + chars
  data = {"book_data": book_data, 
          "chars": chars, 
          "data_len" : len(chars), 
          "char_to_ind": OrderedDict((char, index) for index, char in enumerate(chars)),
          "ind_to_char": OrderedDict((index, char) for index, char in enumerate(chars))
          }
  return data

def CharToInd(dict, input):
  pass

def IndToChar(dict, output):
  pass

data = LoadText("/content/drive/MyDrive/DirName/Datasets/goblet_book.txt")
print(data["char_to_ind"])
print(len(data["char_to_ind"]))

class RNN():
  def __init__(self, data, m = 100, seq_length = 25, eta = 0.1, sig = 0.01, epochs = 10, 
               optimizer = None, plot = False, synthesize = True):
    self.data = data
    self.book_data = data["book_data"]
    self.chars = data["chars"]
    self.K = data["data_len"]
    self.m = m

    self.eta = eta
    self.sig = sig
    self.epochs = epochs
    self.seq_length = seq_length

    self.b = cp.zeros((m, 1))
    self.c = cp.zeros((data["data_len"], 1))
    self.U = cp.random.normal(0, sig, size=(m, self.K))
    self.W = cp.random.normal(0, sig, size=(m, m))
    self.V = cp.random.normal(0, sig, size=(self.K, m))

    self.e = 0

    self.plot = plot
    self.synthesize = synthesize

  def char_to_ind(self, char):
    ind = cp.zeros((self.K, len(char)))
    for i in range(len(char)):
      ind[self.data["char_to_ind"][char[i]], i] = 1
    return ind

  def ind_to_char(self, ind):
    char = ""
    # if type(ind) == cp._core.core.ndarray:
    if type(ind) == np.ndarray:
      ind = cp.argmax(ind, axis = 0).tolist()
    else:
      ind = [ind]
    # print(type(ind))
    for i in ind:
      char += self.data["ind_to_char"][i] 
    return char

  def p_to_text(self, p):
    # if type(p) == cp._core.core.ndarray:
    if type(p) == np.ndarray:
      ind = int(cp.random.choice(range(self.K), size = 1, p=cp.reshape(p,(p.shape[0]))))
      char = self.ind_to_char(ind)
      return char
    else:
      text = ""
      for i in p:
        ind = int(cp.random.choice(range(self.K), size = 1, p=cp.reshape(i,(p[0].shape[0]))))
        char = self.ind_to_char(ind)
        text += char
      return text
  
  def synthesize_text(self, n, x, h):
    text = ""
    for i in range(n):
      h, p = self.evaluate_classifier(h, x)
      ind = cp.random.choice(range(self.K), p=p.flat)
      # print(ind)
      x = cp.zeros((self.K, 1))
      x[ind, 0] = 1

      char = self.ind_to_char(ind)
      text += char

      # text += self.p_to_text(p)

    return text
  # def synthesize_text(self, h, xnext, n):
  #   # xnext = np.zeros((self.vocab_len, 1))
  #   # xnext[ix] = 1 

  #   txt = ''
  #   for t in range(n):
  #       h, p = self.evaluate_classifier(h, xnext)
  #       # Sample from the vocabulary based on the flattened probability
  #       # vector p and the uniform distribution
  #       ix = np.random.choice(range(self.vocab_len), p=p.flat)
  #       xnext = np.zeros((self.vocab_len, 1))
  #       xnext[ix] = 1 # 1-hot-encoding
  #       txt += self.ind_to_char[ix]

  #   return txt


  def softmax(self, S):
    return cp.exp(S) / cp.sum(cp.exp(S), axis = 0)

  def tanh(self, a):
    return cp.tanh(a)

  def evaluate_classifier(self, h0, X):
    a = self.W @ h0 + self.U @ X + self.b
    h = self.tanh(a)
    o = self.V @ h + self.c
    p = self.softmax(o)

    return h, p


  def forward(self, h0):
    h_dic, p_dic = {-1:h0}, {}
    for i in range(self.seq_length):
      # print("forward x:", self.book_data[self.e])
      x = self.char_to_ind(self.book_data[self.e])
      self.e += 1

      if i == 0:
        h = h0
      h, p = self.evaluate_classifier(h, x)
      h_dic[i] = h
      p_dic[i] = p
      # h_list.append(h)
      # p_list.append(p)
    return h_dic, p_dic

  def backward(self, h, p):
    G_U, G_V, G_W, G_b, G_c, h_next = 0, 0, 0, 0, 0, 0
    for i in reversed(range(self.seq_length)):
      # print("backward x:", self.book_data[self.e - self.seq_length + i])
      # print("backward y:", self.book_data[self.e - self.seq_length + i + 1])
      x = self.char_to_ind(self.book_data[self.e - self.seq_length + i])
      y = self.char_to_ind(self.book_data[self.e - self.seq_length + i + 1])
      G_o = p[i] - y
      G_V += G_o @ h[i].T
      G_c += G_o

      G_h = self.V.T @ G_o + h_next
      G_a = G_h * (1 - cp.square(h[i]))

      G_U += G_a @ x.T
      G_W += G_a @ h[i - 1].T
      G_b += G_a

      h_next = self.W.T @ G_a

    G_U = cp.clip(G_U, -5, 5)
    G_V = cp.clip(G_V, -5, 5)
    G_W = cp.clip(G_W, -5, 5)
    G_b = cp.clip(G_b, -5, 5)
    G_c = cp.clip(G_c, -5, 5)

    return G_U, G_V, G_W, G_b, G_c

  def compute_loss(self, p):
    loss = 0
    for i in range(self.seq_length):
      x = self.char_to_ind(self.book_data[self.e - self.seq_length + i])
      y = self.char_to_ind(self.book_data[self.e - self.seq_length + i + 1])
      loss += -cp.sum(cp.log(p[i]) * y)

    return loss

  def PlotPerformance(self, n_epochs, costs_training, title = ""):
    epochs = (cp.arange(n_epochs))

    fig, ax = plt.subplots(figsize=(10, 8))
    ax.plot(epochs, costs_training, label="Training set")
    # ax.plot(epochs, costs_val, label="Validation set")
    ax.legend()
    ax.set(xlabel='Update step', ylabel=title)
    ax.grid()   

  def mini_batch_GD(self):
    G_b_mem, G_c_mem, G_U_mem, G_V_mem, G_W_mem, count = 0, 0, 0, 0, 0, 0
    loss_plot = []
    for epoch in range(self.epochs):
      print("epoch:", epoch)
      self.e = 0
      for batch_num in range(int((len(self.book_data) - 1) / self.seq_length)):
        if batch_num == 0:
          h, p = self.forward(cp.zeros((self.m,1)))
        else:
          h, p = self.forward(h[self.seq_length - 1])
        G_U, G_V, G_W, G_b, G_c = self.backward(h, p)
        G_b_mem += G_b * G_b
        G_c_mem += G_c * G_c
        G_U_mem += G_U * G_U
        G_V_mem += G_V * G_V
        G_W_mem += G_W * G_W

        self.b -= self.eta / cp.sqrt(G_b_mem + cp.finfo(float).eps) * G_b
        self.c -= self.eta / cp.sqrt(G_c_mem + cp.finfo(float).eps) * G_c
        self.U -= self.eta / cp.sqrt(G_U_mem + cp.finfo(float).eps) * G_U
        self.V -= self.eta / cp.sqrt(G_V_mem + cp.finfo(float).eps) * G_V
        self.W -= self.eta / cp.sqrt(G_W_mem + cp.finfo(float).eps) * G_W

        loss = self.compute_loss(p)
        if count == 0 and epoch == 0: 
          smooth_loss = loss
        smooth_loss = 0.999 * smooth_loss + 0.001 * loss

        if self.plot and count % 100 == 0:
          loss_plot.append(smooth_loss)

        if count % 10000 == 0:
          # break
          print("Iteration:", count, "loss:", smooth_loss)

        if count % 10000 and self.synthesize:
          # print(h)
          print(self.synthesize_text(200, self.char_to_ind(self.book_data[self.e]), h[self.seq_length - 1]))

        count += 1
    if self.plot:
      loss_plot = cp.array(loss_plot)
      self.PlotPerformance(loss_plot.shape[0], loss_plot, "Loss")

rnn_lbx = RNN(data, plot = True, epochs = 2, synthesize = False)
rnn_lbx.mini_batch_GD()

start = "G"
txt = rnn_lbx.synthesize_text(1000, rnn_lbx.char_to_ind(start), cp.zeros((rnn_lbx.m,1)))
print(start + txt)